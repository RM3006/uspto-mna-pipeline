terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    random = {
      source  = "hashicorp/random"
      version = "~> 3.0"
    }
  }
}

provider "aws" {
  region = "us-east-1"
}

# 1. Generate random suffix to ensure unique bucket naming
resource "random_id" "bucket_suffix" {
  byte_length = 4
}

# 2. Create S3 bucket for data lake storage
resource "aws_s3_bucket" "data_lake" {
  bucket        = "uspto-data-lake-${random_id.bucket_suffix.hex}"
  force_destroy = true
}

# 3. Enable versioning on the S3 bucket
resource "aws_s3_bucket_versioning" "data_lake_ver" {
  bucket = aws_s3_bucket.data_lake.id
  versioning_configuration {
    status = "Enabled"
  }
}

# 4. Configure public access block to secure the bucket
resource "aws_s3_bucket_public_access_block" "block_public" {
  bucket                  = aws_s3_bucket.data_lake.id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# 5. Create IAM user for Python data ingestion
resource "aws_iam_user" "uploader" {
  name = "uspto_python_uploader"
}

# 6. Define and attach policy to allow write access to the data lake
resource "aws_iam_policy" "s3_write_policy" {
  name        = "uspto_s3_write_access"
  description = "Allows writing to the USPTO data lake bucket"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:PutObject",
          "s3:GetObject",
          "s3:ListBucket"
        ]
        Resource = [
          aws_s3_bucket.data_lake.arn,
          "${aws_s3_bucket.data_lake.arn}/*"
        ]
      }
    ]
  })
}

resource "aws_iam_user_policy_attachment" "attach_s3_write" {
  user       = aws_iam_user.uploader.name
  policy_arn = aws_iam_policy.s3_write_policy.arn
}

# 7. Generate local environment file with Terraform outputs
resource "local_file" "generated_env" {
  content = <<EOT
# This file is auto-generated by Terraform.
AWS_BUCKET_NAME=${aws_s3_bucket.data_lake.bucket}
AWS_IAM_USER_NAME=${aws_iam_user.uploader.name}
AWS_IAM_SNOWFLAKE_ROLE=${aws_iam_role.snowflake_reader.arn}
# Note: Snowflake Role ARN is now handled in iam.tf output
EOT
  filename = "${path.module}/../../.env.generated"
}

# 8. Output the final bucket name for reference
output "final_bucket_name" {
  value = aws_s3_bucket.data_lake.bucket
}