terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    # The Random Provider
    random = {
      source  = "hashicorp/random"
      version = "~> 3.0"
    }
  }
}

provider "aws" {
  region = "us-east-1"
}

# 1. Generate a Random Suffix (Guarantees Uniqueness)
resource "random_id" "bucket_suffix" {
  byte_length = 4 # Generates an 8-character hex string (e.g., a1b2c3d4)
}

# 2. The Data Lake Bucket
resource "aws_s3_bucket" "data_lake" {
  # Name format: uspto-data-lake-[random-string]
  bucket = "uspto-data-lake-${random_id.bucket_suffix.hex}"
  
  force_destroy = true
}

# 3. Versioning
resource "aws_s3_bucket_versioning" "data_lake_ver" {
  bucket = aws_s3_bucket.data_lake.id
  versioning_configuration {
    status = "Enabled"
  }
}

# 4. Security Block
resource "aws_s3_bucket_public_access_block" "block_public" {
  bucket = aws_s3_bucket.data_lake.id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# 5. The IAM User (The Robot)
resource "aws_iam_user" "uploader" {
  name = "uspto_python_uploader"
}

# 6. The IAM Policy (Permissions)
resource "aws_iam_policy" "s3_write_policy" {
  name        = "uspto_s3_write_access"
  description = "Allows writing to the USPTO data lake bucket"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:PutObject",
          "s3:GetObject",
          "s3:ListBucket"
        ]
        Resource = [
          aws_s3_bucket.data_lake.arn,
          "${aws_s3_bucket.data_lake.arn}/*"
        ]
      }
    ]
  })
}

# Attach Policy
resource "aws_iam_user_policy_attachment" "attach_s3_write" {
  user       = aws_iam_user.uploader.name
  policy_arn = aws_iam_policy.s3_write_policy.arn
}

# 7. Outputs (CRITICAL: This tells you what name was generated)
output "final_bucket_name" {
  description = "Copy this value to your .env file as AWS_BUCKET_NAME"
  value       = aws_s3_bucket.data_lake.bucket
}

output "iam_user_name" {
  value = aws_iam_user.uploader.name
}

# 8. Get current account ID
data "aws_caller_identity" "current" {}

# 10. Create Policy to read S3 bucket
resource "aws_iam_policy" "snowflake_read_policy" {
  name        = "uspto_snowflake_read_access"
  
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:GetObjectVersion",
          "s3:ListBucket"
        ]
        Resource = [
          aws_s3_bucket.data_lake.arn,
          "${aws_s3_bucket.data_lake.arn}/*"
        ]
      }
    ]
  })
}

# 11. Attach Policy to Role
resource "aws_iam_role_policy_attachment" "attach_read_snowflake" {
  role       = aws_iam_role.snowflake_reader.name
  policy_arn = aws_iam_policy.snowflake_read_policy.arn
}

# 12. Output Role ARN for Snowflake usage
output "snowflake_role_arn" {
  value = aws_iam_role.snowflake_reader.arn
}

# 13. AUTOMATION: Write infrastructure details to a local file
resource "local_file" "generated_env" {
  content = <<EOT
# This file is auto-generated by Terraform.
AWS_BUCKET_NAME=${aws_s3_bucket.data_lake.bucket}
AWS_IAM_USER_NAME=${aws_iam_user.uploader.name}
AWS_IAM_SNOWFLAKE_ROLE=${aws_iam_role.snowflake_reader.arn}
EOT
  
  # Save it to the project root (2 levels up from infrastructure/aws)
  filename = "${path.module}/../../.env.generated"
}